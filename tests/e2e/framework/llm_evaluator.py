import json
from typing import Any
from semantic_kernel import Kernel
from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion, OpenAIChatPromptExecutionSettings
from semantic_kernel.agents import ChatCompletionAgent
from semantic_kernel.functions import KernelArguments
from config.settings import get_settings
from .e2e_test_models import EvaluationResult

class LLMEvaluator:
    """Generic LLM-based output evaluator"""
    
    def __init__(self):
        self.settings = get_settings()
        self.agent = None
    
    async def initialize(self):
        """Initialize the evaluator agent"""
        kernel = Kernel()
        
        execution_settings = OpenAIChatPromptExecutionSettings(
            response_format=EvaluationResult,
            temperature=0.1  # Low temperature for consistent evaluation
        )
        
        chat_service = AzureChatCompletion(
            deployment_name=self.settings.azure_openai_deployment_name,
            api_key=self.settings.azure_openai_api_key,
            endpoint=self.settings.azure_openai_endpoint,
            api_version=self.settings.azure_openai_api_version,
        )
        
        kernel.add_service(chat_service)
        
        self.agent = ChatCompletionAgent(
            service=chat_service,
            kernel=kernel,
            name="TestEvaluatorAgent",
            arguments=KernelArguments(execution_settings),
            instructions="""You are a Test Evaluator Agent that compares actual outputs against expected outputs.

Your task is to determine if the actual output is semantically equivalent to the expected output.

Evaluation guidelines:
1. Focus on semantic equivalence, not exact string matching
2. Ignore formatting differences (whitespace, newlines, indentation)
3. Ignore order differences in lists/arrays if order is not semantically important
4. Consider null/None/empty as equivalent when semantically the same
5. For JSON structures, compare the logical content
6. For text outputs, compare the meaning and key information

Provide:
- is_equivalent: true if outputs are semantically equivalent
- confidence: 0.0 to 1.0 score of your confidence
- reasoning: clear explanation of your evaluation
- differences: list of important differences found (if any)
- suggestions: improvements for the actual output (if any)

Be strict but fair - outputs should convey the same information to be considered equivalent."""
        )
    
    async def evaluate(self, actual_output: Any, expected_output: Any, 
                       evaluation_criteria: list = None) -> EvaluationResult:
        """
        Evaluate if actual output is semantically equivalent to expected output
        
        Args:
            actual_output: The output generated by the agent
            expected_output: The expected output
            evaluation_criteria: Optional specific criteria for evaluation
        
        Returns:
            EvaluationResult with detailed evaluation
        """
        
        # Convert to JSON strings for comparison
        actual_json = json.dumps(actual_output, indent=2) if not isinstance(actual_output, str) else actual_output
        expected_json = json.dumps(expected_output, indent=2) if not isinstance(expected_output, str) else expected_output
        
        criteria_text = ""
        if evaluation_criteria:
            criteria_text = f"\n\nAdditional evaluation criteria:\n" + "\n".join(f"- {c}" for c in evaluation_criteria)
        
        prompt = f"""Compare the actual output against the expected output and determine if they are semantically equivalent.

EXPECTED OUTPUT:
```json
{expected_json}
```

ACTUAL OUTPUT:
```json
{actual_json}
```
{criteria_text}

Evaluate if these outputs are semantically equivalent."""
        
        response = await self.agent.get_response(prompt)
        result = EvaluationResult.model_validate(json.loads(response.message.content))
        return result